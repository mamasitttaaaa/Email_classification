{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f6650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 8446.68 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 10952.90 examples/s]\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1689\n",
      "  Number of trainable parameters = 109953826\n",
      "  0%|          | 0/1689 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 30%|██▉       | 500/1689 [32:35<1:10:02,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5391, 'learning_rate': 1.4079336885731204e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 563/1689 [36:14<55:22,  2.95s/it]  The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "                                                  \n",
      " 33%|███▎      | 563/1689 [37:15<55:22,  2.95s/it]Saving model checkpoint to ./results/checkpoint-563\n",
      "Configuration saved in ./results/checkpoint-563/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5100831985473633, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 61.0321, 'eval_samples_per_second': 8.192, 'eval_steps_per_second': 1.032, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-563/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-563/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-563/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 59%|█████▉    | 1000/1689 [59:46<28:44,  2.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5218, 'learning_rate': 8.158673771462404e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1126/1689 [1:05:00<20:08,  2.15s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "                                                     \n",
      " 67%|██████▋   | 1126/1689 [1:05:44<20:08,  2.15s/it]Saving model checkpoint to ./results/checkpoint-1126\n",
      "Configuration saved in ./results/checkpoint-1126/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5064492225646973, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 44.7585, 'eval_samples_per_second': 11.171, 'eval_steps_per_second': 1.408, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1126/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1126/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1126/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 89%|████████▉ | 1500/1689 [1:21:20<07:49,  2.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5147, 'learning_rate': 2.238010657193606e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1689/1689 [1:29:10<00:00,  2.15s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "                                                     \n",
      "100%|██████████| 1689/1689 [1:29:53<00:00,  2.15s/it]Saving model checkpoint to ./results/checkpoint-1689\n",
      "Configuration saved in ./results/checkpoint-1689/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.50480318069458, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 43.6909, 'eval_samples_per_second': 11.444, 'eval_steps_per_second': 1.442, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1689/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1689/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1689/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-563 (score: 0.0021534625410857986).\n",
      "100%|██████████| 1689/1689 [1:29:56<00:00,  3.20s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5396.4909, 'train_samples_per_second': 2.502, 'train_steps_per_second': 0.313, 'train_loss': 3.524956874271481, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.47it/s]\n",
      "Configuration saved in gbert_email_classifier/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5100831985473633, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 43.4857, 'eval_samples_per_second': 11.498, 'eval_steps_per_second': 1.449, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to: gbert_email_classifier\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "model_name = \"deepset/gbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "train_ds = train_ds.map(preprocess_function, batched=True)\n",
    "test_ds = test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "save_dir = \"gbert_email_classifier\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.pkl\"))\n",
    "\n",
    "print(f\"Trained model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9661156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.71      0.65        14\n",
      "           1       0.62      0.62      0.62        13\n",
      "           2       0.71      0.53      0.61        19\n",
      "           3       0.79      0.73      0.76        15\n",
      "           4       0.77      0.77      0.77        13\n",
      "           5       0.88      0.88      0.88        17\n",
      "           6       0.60      0.83      0.70        18\n",
      "           7       0.76      0.84      0.80        19\n",
      "           8       0.71      0.83      0.77        12\n",
      "           9       0.75      0.75      0.75        16\n",
      "          10       0.43      0.43      0.43        14\n",
      "          11       0.92      0.73      0.81        15\n",
      "          12       0.92      0.71      0.80        17\n",
      "          13       0.56      0.71      0.62        14\n",
      "          14       0.85      0.69      0.76        16\n",
      "          15       0.81      0.68      0.74        19\n",
      "          16       0.69      0.90      0.78        10\n",
      "          17       0.71      0.71      0.71        14\n",
      "          18       0.78      0.82      0.80        17\n",
      "          19       0.57      0.57      0.57        14\n",
      "          20       0.54      0.50      0.52        14\n",
      "          21       0.76      0.87      0.81        15\n",
      "          22       0.79      0.73      0.76        15\n",
      "          23       1.00      0.43      0.60         7\n",
      "          24       0.83      0.91      0.87        11\n",
      "          25       0.67      0.62      0.64        13\n",
      "          26       1.00      0.67      0.80        15\n",
      "          27       0.62      0.84      0.71        19\n",
      "          28       0.90      0.69      0.78        13\n",
      "          29       0.82      0.74      0.78        19\n",
      "          30       0.69      0.82      0.75        11\n",
      "          31       0.60      0.88      0.71        17\n",
      "          32       0.92      0.65      0.76        17\n",
      "          33       0.71      0.62      0.67         8\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.74      0.72      0.72       500\n",
      "weighted avg       0.74      0.72      0.72       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + LogisticRegression baseline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vec = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_train = vec.fit_transform(train_df['text'])\n",
    "X_test  = vec.transform(test_df['text'])\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "clf.fit(X_train, train_df['label'])\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(test_df['label'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f34cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test shapes: (4500, 2) (500, 2)\n",
      "Classes in encoder: 34\n",
      "num_labels = 34\n",
      "Train label distribution:\n",
      " label\n",
      "0     130\n",
      "1     121\n",
      "2     171\n",
      "3     136\n",
      "4     115\n",
      "5     154\n",
      "6     159\n",
      "7     171\n",
      "8     111\n",
      "9     143\n",
      "10    127\n",
      "11    138\n",
      "12    150\n",
      "13    127\n",
      "14    140\n",
      "15    169\n",
      "16     86\n",
      "17    124\n",
      "18    159\n",
      "19    130\n",
      "20    130\n",
      "21    133\n",
      "22    137\n",
      "23     64\n",
      "24     96\n",
      "25    115\n",
      "26    133\n",
      "27    170\n",
      "28    114\n",
      "29    172\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 7252.18 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 7513.50 examples/s]\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n",
      "Test columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2252\n",
      "  Number of trainable parameters = 109953826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity logits shape: torch.Size([8, 34])  expected second dim: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2252 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 22%|██▏       | 500/2252 [24:04<1:14:09,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1415, 'learning_rate': 1.6559546313799624e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 563/2252 [26:43<1:00:29,  2.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      " 25%|██▌       | 563/2252 [27:23<1:00:29,  2.15s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-563\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-563/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6522839069366455, 'eval_accuracy': 0.69, 'eval_f1_macro': 0.6385645170624866, 'eval_runtime': 40.9014, 'eval_samples_per_second': 12.225, 'eval_steps_per_second': 0.782, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-563/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-563/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-563/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 44%|████▍     | 1000/2252 [45:38<51:37,  2.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2636, 'learning_rate': 1.1833648393194708e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1126/2252 [50:49<40:16,  2.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      " 50%|█████     | 1126/2252 [51:30<40:16,  2.15s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-1126\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-1126/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6033123731613159, 'eval_accuracy': 0.83, 'eval_f1_macro': 0.8158698550175251, 'eval_runtime': 40.8125, 'eval_samples_per_second': 12.251, 'eval_steps_per_second': 0.784, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-1126/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-1126/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-1126/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 67%|██████▋   | 1500/2252 [1:07:06<31:08,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5546, 'learning_rate': 7.107750472589793e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1689/2252 [1:14:58<20:25,  2.18s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      " 75%|███████▌  | 1689/2252 [1:15:39<20:25,  2.18s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-1689\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-1689/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37991684675216675, 'eval_accuracy': 0.878, 'eval_f1_macro': 0.8729938101437713, 'eval_runtime': 40.9525, 'eval_samples_per_second': 12.209, 'eval_steps_per_second': 0.781, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-1689/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-1689/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-1689/special_tokens_map.json\n",
      "Deleting older checkpoint [gbert_email_classifier_fixed/checkpoint-563] due to args.save_total_limit\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 89%|████████▉ | 2000/2252 [1:28:39<10:24,  2.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3838, 'learning_rate': 2.3818525519848773e-06, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [1:39:06<00:00,  2.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "100%|██████████| 2252/2252 [1:39:47<00:00,  2.15s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-2252\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-2252/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3391866385936737, 'eval_accuracy': 0.882, 'eval_f1_macro': 0.8818058479706438, 'eval_runtime': 41.194, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.777, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-2252/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-2252/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-2252/special_tokens_map.json\n",
      "Deleting older checkpoint [gbert_email_classifier_fixed/checkpoint-1126] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from gbert_email_classifier_fixed/checkpoint-2252 (score: 0.8818058479706438).\n",
      "100%|██████████| 2252/2252 [1:39:50<00:00,  2.66s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5990.2027, 'train_samples_per_second': 3.005, 'train_steps_per_second': 0.376, 'train_loss': 1.222945096437715, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:40<00:00,  1.26s/it]\n",
      "Saving model checkpoint to gbert_email_classifier_fixed\n",
      "Configuration saved in gbert_email_classifier_fixed/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 0.3391866385936737, 'test_accuracy': 0.882, 'test_f1_macro': 0.8818058479706438, 'test_runtime': 41.7755, 'test_samples_per_second': 11.969, 'test_steps_per_second': 0.766}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     0.9333    1.0000    0.9655        14\n",
      "                   Account clarification     1.0000    0.7692    0.8696        13\n",
      "                          Bank statement     0.7143    0.7895    0.7500        19\n",
      "                            Calculations     0.9286    0.8667    0.8966        15\n",
      "                               Call-back     0.9286    1.0000    0.9630        13\n",
      "                            Cancellation     0.9412    0.9412    0.9412        17\n",
      "                    Change customer data     0.9474    1.0000    0.9730        18\n",
      "          Change in payment transactions     0.9048    1.0000    0.9500        19\n",
      "                       Claims management     0.9167    0.9167    0.9167        12\n",
      "                    Compensation release     0.8333    0.9375    0.8824        16\n",
      "                               Complaint     0.9286    0.9286    0.9286        14\n",
      "                     Contract adjustment     0.8235    0.9333    0.8750        15\n",
      "                      Contract rewriting     0.8889    0.9412    0.9143        17\n",
      "                             Credit item     0.8462    0.7857    0.8148        14\n",
      "           Damage report/repair approval     0.9231    0.7500    0.8276        16\n",
      "                            Data extract     0.9412    0.8421    0.8889        19\n",
      "                           Devinculation     1.0000    1.0000    1.0000        10\n",
      "                     Duplicate/KSV block     1.0000    0.9286    0.9630        14\n",
      "             FB Lustum/ beneficial owner     1.0000    0.9412    0.9697        17\n",
      "                Financing balance credit     0.5625    0.6429    0.6000        14\n",
      "Financing balance residual value leasing     0.8000    0.5714    0.6667        14\n",
      "                      Follow-up Contract     0.8667    0.8667    0.8667        15\n",
      "                   Fuel card/maintenance     0.9333    0.9333    0.9333        15\n",
      "            General contract information     1.0000    0.7143    0.8333         7\n",
      "                  General enquiry/errors     0.9000    0.8182    0.8571        11\n",
      "                                HB & HBO     0.7500    0.6923    0.7200        13\n",
      "                        Insurance change     1.0000    0.7333    0.8462        15\n",
      "                         Insurance offer     0.7391    0.8947    0.8095        19\n",
      "                       Liability Insurer     1.0000    0.9231    0.9600        13\n",
      "                Policy/endorsement/cover     0.7727    0.8947    0.8293        19\n",
      "                              Sanierung      1.0000    1.0000    1.0000        11\n",
      "                    Third party purchase     0.8095    1.0000    0.8947        17\n",
      "                             Unsubscribe     1.0000    1.0000    1.0000        17\n",
      "                             Vinculation     0.8750    0.8750    0.8750         8\n",
      "\n",
      "                                accuracy                         0.8820       500\n",
      "                               macro avg     0.8944    0.8774    0.8818       500\n",
      "                            weighted avg     0.8888    0.8820    0.8815       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/special_tokens_map.json\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer and label_encoder saved to gbert_email_classifier_fixed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "PREPARED_DIR = \"prepared_data\" \n",
    "MODEL_NAME = \"deepset/gbert-base\"\n",
    "OUTPUT_DIR = \"gbert_email_classifier_fixed\"\n",
    "MAX_LENGTH = 256\n",
    "RANDOM_SEED = 42\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "print(\"Loaded train/test shapes:\", train_df.shape, test_df.shape)\n",
    "print(\"Classes in encoder:\", len(label_encoder.classes_))\n",
    "\n",
    "if train_df['label'].dtype != int and not np.issubdtype(train_df['label'].dtype, np.integer):\n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "if test_df['label'].dtype != int and not np.issubdtype(test_df['label'].dtype, np.integer):\n",
    "    test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(\"num_labels =\", num_labels)\n",
    "print(\"Train label distribution:\\n\", train_df['label'].value_counts().sort_index().head(30))\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "if \"label\" in hf_train.column_names:\n",
    "    hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "if \"label\" in hf_test.column_names:\n",
    "    hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "cols_to_remove = [c for c in hf_train.column_names if c not in keep_cols]\n",
    "if cols_to_remove:\n",
    "    hf_train = hf_train.remove_columns(cols_to_remove)\n",
    "    hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Train columns after preprocess:\", hf_train.column_names)\n",
    "print(\"Test columns after preprocess:\", hf_test.column_names)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.eval()\n",
    "sample = hf_train.select(range(min(8, len(hf_train)))).with_format(\"pt\")\n",
    "batch = {k: sample[k] for k in (\"input_ids\", \"attention_mask\")}\n",
    "with torch.no_grad():\n",
    "    out = model(**batch)\n",
    "print(\"Sanity logits shape:\", out.logits.shape, \" expected second dim:\", num_labels)\n",
    "if out.logits.shape[1] != num_labels:\n",
    "    raise RuntimeError(\"logits second dim != num_labels; check num_labels and model init\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    warmup_ratio=0.06,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "print(\"\\nClassification report on TEST:\")\n",
    "print(classification_report(hf_test[\"labels\"], preds, target_names=label_encoder.classes_, digits=4))\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "print(\"Model, tokenizer and label_encoder saved to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3052d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels = 34\n",
      "Train label counts (first 10 indices):\n",
      " label\n",
      "0    130\n",
      "1    121\n",
      "2    171\n",
      "3    136\n",
      "4    115\n",
      "5    154\n",
      "6    159\n",
      "7    171\n",
      "8    111\n",
      "9    143\n",
      "Name: count, dtype: int64\n",
      "Final sizes (train/val/test): 3825 675 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 3825/3825 [00:00<00:00, 7295.19 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 7664.18 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 8281.15 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Class weights (first 10): [0.9697186  1.0356218  0.7356486  0.9195606  1.0884596  0.81426746\n",
      " 0.79014105 0.7356486  1.1347771  0.87433636]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_35306/3499180351.py:180: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "  0%|          | 0/2395 [19:04<?, ?it/s]\n",
      "  0%|          | 0/2395 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 20%|██        | 479/2395 [24:52<1:38:47,  3.09s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                    \n",
      " 20%|██        | 479/2395 [25:58<1:38:47,  3.09s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8954533338546753, 'eval_accuracy': 0.6444444444444445, 'eval_f1_macro': 0.625800100373603, 'eval_runtime': 65.4561, 'eval_samples_per_second': 10.312, 'eval_steps_per_second': 0.657, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 21%|██        | 500/2395 [27:09<1:40:34,  3.18s/it] \n",
      " 21%|██        | 500/2395 [27:10<1:40:34,  3.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0465, 'grad_norm': 0.02386416308581829, 'learning_rate': 1.6836961350510885e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 958/2395 [50:24<56:00,  2.34s/it]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                    \n",
      " 40%|████      | 958/2395 [51:27<56:00,  2.34s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7222554683685303, 'eval_accuracy': 0.8533333333333334, 'eval_f1_macro': 0.8416473750645744, 'eval_runtime': 63.4404, 'eval_samples_per_second': 10.64, 'eval_steps_per_second': 0.678, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 42%|████▏     | 1000/2395 [53:43<1:09:57,  3.01s/it]\n",
      " 42%|████▏     | 1000/2395 [53:43<1:09:57,  3.01s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2144, 'grad_norm': 0.3782365918159485, 'learning_rate': 1.2394491337183474e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 1437/2395 [1:19:52<1:15:40,  4.74s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                       \n",
      " 60%|██████    | 1437/2395 [1:22:30<1:15:40,  4.74s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.47410309314727783, 'eval_accuracy': 0.88, 'eval_f1_macro': 0.8740040858147461, 'eval_runtime': 158.0409, 'eval_samples_per_second': 4.271, 'eval_steps_per_second': 0.272, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 63%|██████▎   | 1500/2395 [1:29:09<2:05:44,  8.43s/it] \n",
      " 63%|██████▎   | 1500/2395 [1:29:10<2:05:44,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5536, 'grad_norm': 0.02386416308581829, 'learning_rate': 7.952021323856065e-06, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1916/2395 [2:12:59<40:25,  5.06s/it]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      " 80%|████████  | 1916/2395 [2:15:13<40:25,  5.06s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3959119915962219, 'eval_accuracy': 0.8844444444444445, 'eval_f1_macro': 0.8757680733715498, 'eval_runtime': 134.7803, 'eval_samples_per_second': 5.008, 'eval_steps_per_second': 0.319, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 84%|████████▎ | 2000/2395 [2:25:28<39:03,  5.93s/it]  \n",
      " 84%|████████▎ | 2000/2395 [2:25:28<39:03,  5.93s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3688, 'grad_norm': 0.3782365918159485, 'learning_rate': 3.5095513105286545e-06, 'epoch': 4.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2395/2395 [3:04:08<00:00,  4.06s/it]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      "100%|██████████| 2395/2395 [3:06:28<00:00,  4.06s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37727463245391846, 'eval_accuracy': 0.8844444444444445, 'eval_f1_macro': 0.8743743591672898, 'eval_runtime': 136.3777, 'eval_samples_per_second': 4.949, 'eval_steps_per_second': 0.315, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2395/2395 [3:06:32<00:00,  4.67s/it] \n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 11192.7079, 'train_samples_per_second': 1.709, 'train_steps_per_second': 0.214, 'train_loss': 1.1327167805649792, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:37<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 0.3963167071342468, 'test_accuracy': 0.876, 'test_f1_macro': 0.8736528445455976, 'test_runtime': 101.8519, 'test_samples_per_second': 4.909, 'test_steps_per_second': 0.314}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     0.8750    1.0000    0.9333        14\n",
      "                   Account clarification     0.6667    0.9231    0.7742        13\n",
      "                          Bank statement     0.7500    0.6316    0.6857        19\n",
      "                            Calculations     0.8235    0.9333    0.8750        15\n",
      "                               Call-back     1.0000    0.9231    0.9600        13\n",
      "                            Cancellation     1.0000    0.8824    0.9375        17\n",
      "                    Change customer data     0.7500    1.0000    0.8571        18\n",
      "          Change in payment transactions     0.9412    0.8421    0.8889        19\n",
      "                       Claims management     0.8462    0.9167    0.8800        12\n",
      "                    Compensation release     0.9333    0.8750    0.9032        16\n",
      "                               Complaint     0.9231    0.8571    0.8889        14\n",
      "                     Contract adjustment     0.8750    0.9333    0.9032        15\n",
      "                      Contract rewriting     0.9412    0.9412    0.9412        17\n",
      "                             Credit item     0.7333    0.7857    0.7586        14\n",
      "           Damage report/repair approval     0.9375    0.9375    0.9375        16\n",
      "                            Data extract     0.9333    0.7368    0.8235        19\n",
      "                           Devinculation     0.7692    1.0000    0.8696        10\n",
      "                     Duplicate/KSV block     0.9286    0.9286    0.9286        14\n",
      "             FB Lustum/ beneficial owner     1.0000    0.9412    0.9697        17\n",
      "                Financing balance credit     0.9000    0.6429    0.7500        14\n",
      "Financing balance residual value leasing     0.8889    0.5714    0.6957        14\n",
      "                      Follow-up Contract     0.9333    0.9333    0.9333        15\n",
      "                   Fuel card/maintenance     0.8333    1.0000    0.9091        15\n",
      "            General contract information     0.7143    0.7143    0.7143         7\n",
      "                  General enquiry/errors     1.0000    0.8182    0.9000        11\n",
      "                                HB & HBO     0.9000    0.6923    0.7826        13\n",
      "                        Insurance change     1.0000    0.7333    0.8462        15\n",
      "                         Insurance offer     0.7500    0.9474    0.8372        19\n",
      "                       Liability Insurer     1.0000    0.9231    0.9600        13\n",
      "                Policy/endorsement/cover     0.8947    0.8947    0.8947        19\n",
      "                              Sanierung      1.0000    0.9091    0.9524        11\n",
      "                    Third party purchase     0.7727    1.0000    0.8718        17\n",
      "                             Unsubscribe     1.0000    1.0000    1.0000        17\n",
      "                             Vinculation     0.8889    1.0000    0.9412         8\n",
      "\n",
      "                                accuracy                         0.8760       500\n",
      "                               macro avg     0.8854    0.8755    0.8737       500\n",
      "                            weighted avg     0.8869    0.8760    0.8747       500\n",
      "\n",
      "Saved model, tokenizer, label_encoder and test report to gbert_email_classifier2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "PREPARED_DIR = \"prepared_data\"\n",
    "OUTPUT_DIR   = \"gbert_email_classifier2\"\n",
    "MODEL_NAME   = \"deepset/gbert-base\"\n",
    "MAX_LENGTH   = 512\n",
    "RANDOM_SEED  = 42\n",
    "NUM_EPOCHS   = 5\n",
    "BATCH_SIZE   = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "VAL_SIZE = 0.15          \n",
    "MIN_VAL_PER_CLASS = 8    \n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    if df[\"label\"].dtype != int and not np.issubdtype(df[\"label\"].dtype, np.integer):\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"num_labels = {num_labels}\")\n",
    "print(\"Train label counts (first 10 indices):\\n\", train_df[\"label\"].value_counts().sort_index().head(10))\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
    "train_idx, val_idx = next(sss.split(train_df[\"text\"], train_df[\"label\"]))\n",
    "train_df_, val_df = train_df.iloc[train_idx].copy(), train_df.iloc[val_idx].copy()\n",
    "\n",
    "val_counts = val_df[\"label\"].value_counts()\n",
    "need_fix = []\n",
    "for lbl in range(num_labels):\n",
    "    c = int(val_counts.get(lbl, 0))\n",
    "    if c < MIN_VAL_PER_CLASS:\n",
    "        need_fix.append((lbl, MIN_VAL_PER_CLASS - c))\n",
    "\n",
    "if need_fix:\n",
    "    adds = []\n",
    "    for lbl, need in need_fix:\n",
    "        pool = train_df_[train_df_[\"label\"] == lbl]\n",
    "        take = min(len(pool), need)\n",
    "        if take > 0:\n",
    "            add_rows = pool.sample(n=take, random_state=RANDOM_SEED)\n",
    "            adds.append(add_rows)\n",
    "            train_df_ = train_df_.drop(add_rows.index)\n",
    "    if adds:\n",
    "        val_df = pd.concat([val_df] + adds, ignore_index=True)\n",
    "\n",
    "train_df_ = train_df_.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "val_df    = val_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Final sizes (train/val/test):\", len(train_df_), len(val_df), len(test_df))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df_.reset_index(drop=True))\n",
    "hf_val   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_val   = hf_val.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "def ensure_labels_name(ds):\n",
    "    return ds.rename_column(\"label\", \"labels\") if \"label\" in ds.column_names else ds\n",
    "\n",
    "hf_train = ensure_labels_name(hf_train)\n",
    "hf_val   = ensure_labels_name(hf_val)\n",
    "hf_test  = ensure_labels_name(hf_test)\n",
    "\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "hf_train = hf_train.remove_columns([c for c in hf_train.column_names if c not in keep_cols])\n",
    "hf_val   = hf_val.remove_columns([c for c in hf_val.column_names if c not in keep_cols])\n",
    "hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Columns:\", hf_train.column_names)\n",
    "\n",
    "counter = Counter(train_df_[\"label\"].tolist())\n",
    "class_counts = np.array([counter.get(i, 0) for i in range(num_labels)], dtype=np.float32)\n",
    "\n",
    "class_weights = (class_counts.sum() / (class_counts + 1e-9))\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class weights (first 10):\", class_weights[:10])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m}\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(\n",
    "            input_ids=inputs.get(\"input_ids\"),\n",
    "            attention_mask=inputs.get(\"attention_mask\")\n",
    "        )\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_t.to(model.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.06,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=RANDOM_SEED,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "\n",
    "report = classification_report(\n",
    "    hf_test[\"labels\"],\n",
    "    preds,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    ")\n",
    "print(\"\\nClassification report on TEST:\\n\", report)\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Saved model, tokenizer, label_encoder and test report to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc4cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels = 34\n",
      "Train label counts (first 10 indices):\n",
      " label\n",
      "0    130\n",
      "1    121\n",
      "2    171\n",
      "3    136\n",
      "4    115\n",
      "5    154\n",
      "6    159\n",
      "7    171\n",
      "8    111\n",
      "9    143\n",
      "Name: count, dtype: int64\n",
      "Final sizes (train/val/test): 3825 675 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3825/3825 [00:00<00:00, 5432.99 examples/s]\n",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 6514.95 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6344.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Class weights (first 10): [0.9847429  1.017655   0.8576996  0.9589372  1.0432926  0.9023677\n",
      " 0.8888988  0.8576996  1.0652591  0.93505955]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_35306/1905109761.py:185: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = WeightedTrainer(\n",
      "  0%|          | 0/2874 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 17%|█▋        | 479/2874 [32:45<2:20:19,  3.52s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      " 17%|█▋        | 479/2874 [34:30<2:20:19,  3.52s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.4096174240112305, 'eval_accuracy': 0.5511111111111111, 'eval_f1_macro': 0.5012342828100883, 'eval_runtime': 105.1913, 'eval_samples_per_second': 6.417, 'eval_steps_per_second': 0.409, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 17%|█▋        | 500/2874 [36:00<2:27:16,  3.72s/it] \n",
      " 17%|█▋        | 500/2874 [36:00<2:27:16,  3.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2942, 'grad_norm': 10.888872146606445, 'learning_rate': 1.8360402165506575e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 958/2874 [1:04:18<1:30:57,  2.85s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      " 33%|███▎      | 958/2874 [1:05:52<1:30:57,  2.85s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9778618812561035, 'eval_accuracy': 0.8503703703703703, 'eval_f1_macro': 0.8314837121107543, 'eval_runtime': 93.2277, 'eval_samples_per_second': 7.24, 'eval_steps_per_second': 0.461, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 35%|███▍      | 1000/2874 [1:09:12<2:26:43,  4.70s/it]\n",
      " 35%|███▍      | 1000/2874 [1:09:13<2:26:43,  4.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5547, 'grad_norm': 12.054339408874512, 'learning_rate': 1.449342614075793e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1437/2874 [1:36:22<1:05:56,  2.75s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                       \n",
      " 50%|█████     | 1437/2874 [1:38:05<1:05:56,  2.75s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.755557119846344, 'eval_accuracy': 0.8755555555555555, 'eval_f1_macro': 0.8660526489997086, 'eval_runtime': 102.1878, 'eval_samples_per_second': 6.605, 'eval_steps_per_second': 0.421, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 52%|█████▏    | 1500/2874 [1:43:09<1:36:34,  4.22s/it] \n",
      " 52%|█████▏    | 1500/2874 [1:43:09<1:36:34,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8004, 'grad_norm': 14.116422653198242, 'learning_rate': 1.062645011600928e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1916/2874 [2:09:50<45:46,  2.87s/it]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      " 67%|██████▋   | 1916/2874 [2:11:25<45:46,  2.87s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7027374505996704, 'eval_accuracy': 0.8874074074074074, 'eval_f1_macro': 0.8795315230004144, 'eval_runtime': 94.5264, 'eval_samples_per_second': 7.141, 'eval_steps_per_second': 0.455, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 70%|██████▉   | 2000/2874 [2:17:39<58:57,  4.05s/it]  \n",
      " 70%|██████▉   | 2000/2874 [2:17:40<58:57,  4.05s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6367, 'grad_norm': 9.249773979187012, 'learning_rate': 6.759474091260635e-06, 'epoch': 4.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2395/2874 [2:42:48<22:52,  2.86s/it]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      " 83%|████████▎ | 2395/2874 [2:44:23<22:52,  2.86s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6857687830924988, 'eval_accuracy': 0.8785185185185185, 'eval_f1_macro': 0.8720539757991187, 'eval_runtime': 94.7595, 'eval_samples_per_second': 7.123, 'eval_steps_per_second': 0.454, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 87%|████████▋ | 2500/2874 [2:53:42<32:58,  5.29s/it]  \n",
      " 87%|████████▋ | 2500/2874 [2:53:43<32:58,  5.29s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5658, 'grad_norm': 7.578507900238037, 'learning_rate': 2.892498066511988e-06, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2874/2874 [3:19:23<00:00,  2.89s/it]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A                                         \n",
      "                                                      \n",
      "100%|██████████| 2874/2874 [3:21:32<00:00,  2.89s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6876186728477478, 'eval_accuracy': 0.8844444444444445, 'eval_f1_macro': 0.8772635645209033, 'eval_runtime': 122.509, 'eval_samples_per_second': 5.51, 'eval_steps_per_second': 0.351, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2874/2874 [3:21:41<00:00,  4.21s/it] \n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 12101.8251, 'train_samples_per_second': 1.896, 'train_steps_per_second': 0.237, 'train_loss': 1.2601150215377224, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:13<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 0.6954149603843689, 'test_accuracy': 0.876, 'test_f1_macro': 0.8732476074913652, 'test_runtime': 76.457, 'test_samples_per_second': 6.54, 'test_steps_per_second': 0.419}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     0.9333    1.0000    0.9655        14\n",
      "                   Account clarification     0.7692    0.7692    0.7692        13\n",
      "                          Bank statement     0.6667    0.7368    0.7000        19\n",
      "                            Calculations     0.8824    1.0000    0.9375        15\n",
      "                               Call-back     1.0000    1.0000    1.0000        13\n",
      "                            Cancellation     1.0000    0.8235    0.9032        17\n",
      "                    Change customer data     0.7500    1.0000    0.8571        18\n",
      "          Change in payment transactions     0.8947    0.8947    0.8947        19\n",
      "                       Claims management     1.0000    0.9167    0.9565        12\n",
      "                    Compensation release     0.8421    1.0000    0.9143        16\n",
      "                               Complaint     0.9286    0.9286    0.9286        14\n",
      "                     Contract adjustment     0.9286    0.8667    0.8966        15\n",
      "                      Contract rewriting     1.0000    0.9412    0.9697        17\n",
      "                             Credit item     0.6875    0.7857    0.7333        14\n",
      "           Damage report/repair approval     0.9333    0.8750    0.9032        16\n",
      "                            Data extract     0.9375    0.7895    0.8571        19\n",
      "                           Devinculation     0.8333    1.0000    0.9091        10\n",
      "                     Duplicate/KSV block     1.0000    0.9286    0.9630        14\n",
      "             FB Lustum/ beneficial owner     1.0000    0.9412    0.9697        17\n",
      "                Financing balance credit     0.8333    0.7143    0.7692        14\n",
      "Financing balance residual value leasing     0.6667    0.4286    0.5217        14\n",
      "                      Follow-up Contract     0.9231    0.8000    0.8571        15\n",
      "                   Fuel card/maintenance     0.8333    1.0000    0.9091        15\n",
      "            General contract information     0.8333    0.7143    0.7692         7\n",
      "                  General enquiry/errors     1.0000    0.7273    0.8421        11\n",
      "                                HB & HBO     0.9000    0.6923    0.7826        13\n",
      "                        Insurance change     1.0000    0.6667    0.8000        15\n",
      "                         Insurance offer     0.7037    1.0000    0.8261        19\n",
      "                       Liability Insurer     1.0000    0.9231    0.9600        13\n",
      "                Policy/endorsement/cover     0.9474    0.9474    0.9474        19\n",
      "                              Sanierung      1.0000    0.9091    0.9524        11\n",
      "                    Third party purchase     0.7391    1.0000    0.8500        17\n",
      "                             Unsubscribe     1.0000    1.0000    1.0000        17\n",
      "                             Vinculation     0.8750    0.8750    0.8750         8\n",
      "\n",
      "                                accuracy                         0.8760       500\n",
      "                               macro avg     0.8895    0.8704    0.8732       500\n",
      "                            weighted avg     0.8868    0.8760    0.8745       500\n",
      "\n",
      "Saved model, tokenizer, label_encoder and test report to gbert_email_classifier3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "# ====== Constants ======\n",
    "PREPARED_DIR = \"prepared_data\"\n",
    "OUTPUT_DIR   = \"gbert_email_classifier3\"\n",
    "MODEL_NAME   = \"deepset/gbert-base\"\n",
    "MAX_LENGTH   = 384\n",
    "RANDOM_SEED  = 42\n",
    "NUM_EPOCHS   = 6\n",
    "BATCH_SIZE   = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "VAL_SIZE = 0.15  \n",
    "MIN_VAL_PER_CLASS = 8 \n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    if df[\"label\"].dtype != int and not np.issubdtype(df[\"label\"].dtype, np.integer):\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"num_labels = {num_labels}\")\n",
    "print(\"Train label counts (first 10 indices):\\n\", train_df[\"label\"].value_counts().sort_index().head(10))\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
    "train_idx, val_idx = next(sss.split(train_df[\"text\"], train_df[\"label\"]))\n",
    "train_df_, val_df = train_df.iloc[train_idx].copy(), train_df.iloc[val_idx].copy()\n",
    "\n",
    "val_counts = val_df[\"label\"].value_counts()\n",
    "need_fix = []\n",
    "for lbl in range(num_labels):\n",
    "    c = int(val_counts.get(lbl, 0))\n",
    "    if c < MIN_VAL_PER_CLASS:\n",
    "        need_fix.append((lbl, MIN_VAL_PER_CLASS - c))\n",
    "\n",
    "if need_fix:\n",
    "    adds = []\n",
    "    for lbl, need in need_fix:\n",
    "        pool = train_df_[train_df_[\"label\"] == lbl]\n",
    "        take = min(len(pool), need)\n",
    "        if take > 0:\n",
    "            add_rows = pool.sample(n=take, random_state=RANDOM_SEED)\n",
    "            adds.append(add_rows)\n",
    "            train_df_ = train_df_.drop(add_rows.index)\n",
    "    if adds:\n",
    "        val_df = pd.concat([val_df] + adds, ignore_index=True)\n",
    "\n",
    "train_df_ = train_df_.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "val_df    = val_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Final sizes (train/val/test):\", len(train_df_), len(val_df), len(test_df))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df_.reset_index(drop=True))\n",
    "hf_val   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_val   = hf_val.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "# Rename 'label' -> 'labels' for Trainer API\n",
    "def ensure_labels_name(ds):\n",
    "    return ds.rename_column(\"label\", \"labels\") if \"label\" in ds.column_names else ds\n",
    "\n",
    "hf_train = ensure_labels_name(hf_train)\n",
    "hf_val   = ensure_labels_name(hf_val)\n",
    "hf_test  = ensure_labels_name(hf_test)\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "hf_train = hf_train.remove_columns([c for c in hf_train.column_names if c not in keep_cols])\n",
    "hf_val   = hf_val.remove_columns([c for c in hf_val.column_names if c not in keep_cols])\n",
    "hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Columns:\", hf_train.column_names)\n",
    "\n",
    "counter = Counter(train_df_[\"label\"].tolist())\n",
    "class_counts = np.array([counter.get(i, 0) for i in range(num_labels)], dtype=np.float32)\n",
    "\n",
    "class_weights = (class_counts.sum() / (class_counts + 1e-9))\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "class_weights = class_weights ** 0.5\n",
    "class_weights_t = torch.tensor(class_weights, dtype=torch.float)\n",
    "print(\"Class weights (first 10):\", class_weights[:10])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m}\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(\n",
    "            input_ids=inputs.get(\"input_ids\"),\n",
    "            attention_mask=inputs.get(\"attention_mask\")\n",
    "        )\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=class_weights_t.to(model.device),\n",
    "            label_smoothing=0.05,   # 0.05–0.1\n",
    "        )\n",
    "        loss = loss_fct(\n",
    "            logits.view(-1, model.config.num_labels),\n",
    "            labels.view(-1)\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.10,          \n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=RANDOM_SEED,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "\n",
    "report = classification_report(\n",
    "    hf_test[\"labels\"],\n",
    "    preds,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    ")\n",
    "print(\"\\nClassification report on TEST:\\n\", report)\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Saved model, tokenizer, label_encoder and test report to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1042a",
   "metadata": {},
   "source": [
    "THE BEST MODEL WITH 0.89 F1-SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30ce789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_labels = 34\n",
      "Train label counts (first 10 indices):\n",
      " label\n",
      "0    130\n",
      "1    121\n",
      "2    171\n",
      "3    136\n",
      "4    115\n",
      "5    154\n",
      "6    159\n",
      "7    171\n",
      "8    111\n",
      "9    143\n",
      "Name: count, dtype: int64\n",
      "Final sizes (train/val/test): 3825 675 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 3825/3825 [00:00<00:00, 5789.55 examples/s]\n",
      "\n",
      "\u001b[A\n",
      "Map: 100%|██████████| 675/675 [00:00<00:00, 6183.42 examples/s]\n",
      "\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 6462.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Class weights (first 10): [0.88838506 0.91807675 1.3927914  0.8651045  0.9412057  0.81407034\n",
      " 0.8019194  0.7737731  0.9610227  0.8435633 ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_35306/1853042861.py:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(*args, **kwargs)\n",
      "  9%|▉         | 266/2874 [2:30:43<24:37:51, 34.00s/it]\n",
      "  0%|          | 0/2874 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 17%|█▋        | 479/2874 [26:53<1:39:17,  2.49s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                     \n",
      " 17%|█▋        | 479/2874 [28:09<1:39:17,  2.49s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.181379795074463, 'eval_accuracy': 0.554074074074074, 'eval_f1_macro': 0.49810002520192154, 'eval_runtime': 76.1074, 'eval_samples_per_second': 8.869, 'eval_steps_per_second': 0.565, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                     \n",
      " 17%|█▋        | 500/2874 [29:26<2:08:46,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2009, 'grad_norm': 10.166940689086914, 'learning_rate': 1.8360402165506575e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 958/2874 [57:15<1:39:45,  3.12s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                    \n",
      "\u001b[A                                     \n",
      " 33%|███▎      | 958/2874 [58:40<1:39:45,  3.12s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.9274799227714539, 'eval_accuracy': 0.8607407407407407, 'eval_f1_macro': 0.848074702018254, 'eval_runtime': 85.561, 'eval_samples_per_second': 7.889, 'eval_steps_per_second': 0.503, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                       \n",
      " 35%|███▍      | 1000/2874 [1:01:24<1:46:10,  3.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4658, 'grad_norm': 10.00593090057373, 'learning_rate': 1.449342614075793e-05, 'epoch': 2.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1437/2874 [1:19:25<35:53,  1.50s/it]  \n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                     \n",
      "\u001b[A                                       \n",
      " 50%|█████     | 1437/2874 [1:20:07<35:53,  1.50s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.742139995098114, 'eval_accuracy': 0.88, 'eval_f1_macro': 0.8710011211420373, 'eval_runtime': 41.7767, 'eval_samples_per_second': 16.157, 'eval_steps_per_second': 1.029, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                       \n",
      " 52%|█████▏    | 1500/2874 [1:22:18<44:19,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7893, 'grad_norm': 14.98968505859375, 'learning_rate': 1.062645011600928e-05, 'epoch': 3.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1916/2874 [1:35:44<23:31,  1.47s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                     \n",
      "\u001b[A                                       \n",
      " 67%|██████▋   | 1916/2874 [1:36:26<23:31,  1.47s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.690744161605835, 'eval_accuracy': 0.8859259259259259, 'eval_f1_macro': 0.8782437082119817, 'eval_runtime': 41.1813, 'eval_samples_per_second': 16.391, 'eval_steps_per_second': 1.044, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                       \n",
      " 70%|██████▉   | 2000/2874 [1:39:07<27:30,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6351, 'grad_norm': 10.422881126403809, 'learning_rate': 6.759474091260635e-06, 'epoch': 4.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 2395/2874 [1:51:33<11:39,  1.46s/it]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                     \n",
      "\u001b[A                                       \n",
      " 83%|████████▎ | 2395/2874 [1:52:14<11:39,  1.46s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6746086478233337, 'eval_accuracy': 0.8859259259259259, 'eval_f1_macro': 0.8792237492484521, 'eval_runtime': 40.6692, 'eval_samples_per_second': 16.597, 'eval_steps_per_second': 1.057, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                       \n",
      " 87%|████████▋ | 2500/2874 [1:55:37<11:40,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5675, 'grad_norm': 7.751171588897705, 'learning_rate': 2.892498066511988e-06, 'epoch': 5.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2874/2874 [2:07:17<00:00,  1.45s/it]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "                                                     \n",
      "\u001b[A                                       \n",
      "100%|██████████| 2874/2874 [2:08:01<00:00,  1.45s/it]\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6762434840202332, 'eval_accuracy': 0.8770370370370371, 'eval_f1_macro': 0.8713009474711272, 'eval_runtime': 40.9726, 'eval_samples_per_second': 16.474, 'eval_steps_per_second': 1.049, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 2874/2874 [2:08:04<00:00,  2.67s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7684.5709, 'train_samples_per_second': 2.987, 'train_steps_per_second': 0.374, 'train_loss': 1.2266799725006252, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:29<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 0.6863285899162292, 'test_accuracy': 0.892, 'test_f1_macro': 0.8938086648760779, 'test_runtime': 30.6814, 'test_samples_per_second': 16.297, 'test_steps_per_second': 1.043}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                           precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     0.9333    1.0000    0.9655        14\n",
      "                   Account clarification     0.8333    0.7692    0.8000        13\n",
      "                          Bank statement     0.7000    0.7368    0.7179        19\n",
      "                            Calculations     0.8333    1.0000    0.9091        15\n",
      "                               Call-back     0.9231    0.9231    0.9231        13\n",
      "                            Cancellation     0.8824    0.8824    0.8824        17\n",
      "                    Change customer data     0.8095    0.9444    0.8718        18\n",
      "          Change in payment transactions     0.9000    0.9474    0.9231        19\n",
      "                       Claims management     1.0000    0.9167    0.9565        12\n",
      "                    Compensation release     0.9375    0.9375    0.9375        16\n",
      "                               Complaint     0.9333    1.0000    0.9655        14\n",
      "                     Contract adjustment     0.8333    1.0000    0.9091        15\n",
      "                      Contract rewriting     0.8333    0.8824    0.8571        17\n",
      "                             Credit item     0.7857    0.7857    0.7857        14\n",
      "           Damage report/repair approval     1.0000    0.8750    0.9333        16\n",
      "                            Data extract     0.9375    0.7895    0.8571        19\n",
      "                           Devinculation     1.0000    1.0000    1.0000        10\n",
      "                     Duplicate/KSV block     1.0000    0.9286    0.9630        14\n",
      "             FB Lustum/ beneficial owner     1.0000    0.9412    0.9697        17\n",
      "                Financing balance credit     0.7692    0.7143    0.7407        14\n",
      "Financing balance residual value leasing     0.7500    0.6429    0.6923        14\n",
      "                      Follow-up Contract     0.9286    0.8667    0.8966        15\n",
      "                   Fuel card/maintenance     0.8750    0.9333    0.9032        15\n",
      "            General contract information     1.0000    0.7143    0.8333         7\n",
      "                  General enquiry/errors     1.0000    0.9091    0.9524        11\n",
      "                                HB & HBO     0.9000    0.6923    0.7826        13\n",
      "                        Insurance change     1.0000    0.8000    0.8889        15\n",
      "                         Insurance offer     0.7826    0.9474    0.8571        19\n",
      "                       Liability Insurer     1.0000    0.9231    0.9600        13\n",
      "                Policy/endorsement/cover     0.9444    0.8947    0.9189        19\n",
      "                              Sanierung      1.0000    1.0000    1.0000        11\n",
      "                    Third party purchase     0.8095    1.0000    0.8947        17\n",
      "                             Unsubscribe     1.0000    1.0000    1.0000        17\n",
      "                             Vinculation     0.8889    1.0000    0.9412         8\n",
      "\n",
      "                                accuracy                         0.8920       500\n",
      "                               macro avg     0.9036    0.8911    0.8938       500\n",
      "                            weighted avg     0.8973    0.8920    0.8913       500\n",
      "\n",
      "Saved model, tokenizer, label_encoder and test report to gbert_email_classifier4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    set_seed\n",
    ")\n",
    "\n",
    "PREPARED_DIR = \"prepared_data\"\n",
    "OUTPUT_DIR   = \"gbert_email_classifier4\"\n",
    "MODEL_NAME   = \"deepset/gbert-base\"\n",
    "MAX_LENGTH   = 384\n",
    "RANDOM_SEED  = 42\n",
    "NUM_EPOCHS   = 6\n",
    "BATCH_SIZE   = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "VAL_SIZE = 0.15 \n",
    "MIN_VAL_PER_CLASS = 8\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "for df in (train_df, test_df):\n",
    "    if df[\"label\"].dtype != int and not np.issubdtype(df[\"label\"].dtype, np.integer):\n",
    "        df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(f\"num_labels = {num_labels}\")\n",
    "print(\"Train label counts (first 10 indices):\\n\", train_df[\"label\"].value_counts().sort_index().head(10))\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=RANDOM_SEED)\n",
    "train_idx, val_idx = next(sss.split(train_df[\"text\"], train_df[\"label\"]))\n",
    "train_df_, val_df = train_df.iloc[train_idx].copy(), train_df.iloc[val_idx].copy()\n",
    "\n",
    "val_counts = val_df[\"label\"].value_counts()\n",
    "need_fix = []\n",
    "for lbl in range(num_labels):\n",
    "    c = int(val_counts.get(lbl, 0))\n",
    "    if c < MIN_VAL_PER_CLASS:\n",
    "        need_fix.append((lbl, MIN_VAL_PER_CLASS - c))\n",
    "\n",
    "if need_fix:\n",
    "    adds = []\n",
    "    for lbl, need in need_fix:\n",
    "        pool = train_df_[train_df_[\"label\"] == lbl]\n",
    "        take = min(len(pool), need)\n",
    "        if take > 0:\n",
    "            add_rows = pool.sample(n=take, random_state=RANDOM_SEED)\n",
    "            adds.append(add_rows)\n",
    "            train_df_ = train_df_.drop(add_rows.index)\n",
    "    if adds:\n",
    "        val_df = pd.concat([val_df] + adds, ignore_index=True)\n",
    "\n",
    "train_df_ = train_df_.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "val_df    = val_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Final sizes (train/val/test):\", len(train_df_), len(val_df), len(test_df))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH\n",
    "    )\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df_.reset_index(drop=True))\n",
    "hf_val   = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_val   = hf_val.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "def ensure_labels_name(ds):\n",
    "    return ds.rename_column(\"label\", \"labels\") if \"label\" in ds.column_names else ds\n",
    "\n",
    "hf_train = ensure_labels_name(hf_train)\n",
    "hf_val   = ensure_labels_name(hf_val)\n",
    "hf_test  = ensure_labels_name(hf_test)\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "hf_train = hf_train.remove_columns([c for c in hf_train.column_names if c not in keep_cols])\n",
    "hf_val   = hf_val.remove_columns([c for c in hf_val.column_names if c not in keep_cols])\n",
    "hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Columns:\", hf_train.column_names)\n",
    "\n",
    "counter = Counter(train_df_[\"label\"].tolist())\n",
    "class_counts = np.array([counter.get(i, 0) for i in range(num_labels)], dtype=np.float32)\n",
    "\n",
    "eps = 1e-6\n",
    "class_weights = 1.0 / np.sqrt(class_counts + eps)\n",
    "\n",
    "hard_classes = [\n",
    "    \"Financing balance residual value leasing\",\n",
    "    \"Financing balance credit\",\n",
    "    \"Bank statement\",\n",
    "    \"Insurance change\",\n",
    "    \"General enquiry/errors\"\n",
    "]\n",
    "idx_map = {cls: int(np.where(label_encoder.classes_ == cls)[0][0]) for cls in hard_classes}\n",
    "for cls, i in idx_map.items():\n",
    "    class_weights[i] *= 1.8\n",
    "\n",
    "class_weights = class_weights / class_weights.mean()\n",
    "\n",
    "class_weights_t_cpu = torch.tensor(class_weights, dtype=torch.float32)\n",
    "print(\"Class weights (first 10):\", class_weights[:10])\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1m = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1m}\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights_cpu: torch.Tensor, label_smoothing: float = 0.05, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._loss_fct = None\n",
    "        self._weights_cpu = class_weights_cpu\n",
    "        self._label_smoothing = label_smoothing\n",
    "\n",
    "    def _get_loss_fct(self):\n",
    "        # лениво создаём ОДИН РАЗ и на правильном устройстве\n",
    "        if self._loss_fct is None:\n",
    "            w = self._weights_cpu.to(self.model.device)\n",
    "            self._loss_fct = nn.CrossEntropyLoss(\n",
    "                weight=w,\n",
    "                label_smoothing=self._label_smoothing\n",
    "            )\n",
    "        return self._loss_fct\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(\n",
    "            input_ids=inputs.get(\"input_ids\"),\n",
    "            attention_mask=inputs.get(\"attention_mask\")\n",
    "        )\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss = self._get_loss_fct()(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.10,\n",
    "    save_total_limit=2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    seed=RANDOM_SEED,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    ")\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights_cpu=class_weights_t_cpu,\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "\n",
    "report = classification_report(\n",
    "    hf_test[\"labels\"],\n",
    "    preds,\n",
    "    target_names=label_encoder.classes_,\n",
    "    digits=4\n",
    ")\n",
    "print(\"\\nClassification report on TEST:\\n\", report)\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "with open(os.path.join(OUTPUT_DIR, \"test_classification_report.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"Saved model, tokenizer, label_encoder and test report to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test shapes: (4500, 2) (500, 2)\n",
      "Classes in encoder: 34\n",
      "num_labels = 34\n",
      "Train label distribution:\n",
      " label\n",
      "0     130\n",
      "1     121\n",
      "2     171\n",
      "3     136\n",
      "4     115\n",
      "5     154\n",
      "6     159\n",
      "7     171\n",
      "8     111\n",
      "9     143\n",
      "10    127\n",
      "11    138\n",
      "12    150\n",
      "13    127\n",
      "14    140\n",
      "15    169\n",
      "16     86\n",
      "17    124\n",
      "18    159\n",
      "19    130\n",
      "20    130\n",
      "21    133\n",
      "22    137\n",
      "23     64\n",
      "24     96\n",
      "25    115\n",
      "26    133\n",
      "27    170\n",
      "28    114\n",
      "29    172\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 11997.54 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 12689.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n",
      "Test columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at deepset/gelectra-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_22527/893037519.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/2252 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 22%|██▏       | 500/2252 [17:42<1:08:29,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5046, 'grad_norm': 1.7636359930038452, 'learning_rate': 1.6559546313799624e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▌       | 563/2252 [20:52<57:15,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3837265968322754, 'eval_accuracy': 0.11, 'eval_f1_macro': 0.030499007490506935, 'eval_runtime': 44.9092, 'eval_samples_per_second': 11.134, 'eval_steps_per_second': 0.713, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 44%|████▍     | 1000/2252 [33:21<25:19,  1.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2722, 'grad_norm': 3.4851925373077393, 'learning_rate': 1.1833648393194708e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1126/2252 [36:16<20:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.962937116622925, 'eval_accuracy': 0.256, 'eval_f1_macro': 0.14457666516490048, 'eval_runtime': 20.9314, 'eval_samples_per_second': 23.888, 'eval_steps_per_second': 1.529, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 67%|██████▋   | 1500/2252 [43:55<15:12,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9613, 'grad_norm': 6.684183120727539, 'learning_rate': 7.107750472589793e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 1689/2252 [48:05<10:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6687939167022705, 'eval_accuracy': 0.328, 'eval_f1_macro': 0.1853267007671209, 'eval_runtime': 20.8458, 'eval_samples_per_second': 23.986, 'eval_steps_per_second': 1.535, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 89%|████████▉ | 2000/2252 [54:26<05:05,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7207, 'grad_norm': 4.933954238891602, 'learning_rate': 2.3818525519848773e-06, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [59:32<00:00,  1.07s/it]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                   \n",
      "100%|██████████| 2252/2252 [59:57<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5524351596832275, 'eval_accuracy': 0.36, 'eval_f1_macro': 0.21359333803229644, 'eval_runtime': 21.1401, 'eval_samples_per_second': 23.652, 'eval_steps_per_second': 1.514, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [59:59<00:00,  1.60s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3599.4051, 'train_samples_per_second': 5.001, 'train_steps_per_second': 0.626, 'train_loss': 3.0628380512894786, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:20<00:00,  1.59it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 2.5524351596832275, 'test_accuracy': 0.36, 'test_f1_macro': 0.21359333803229644, 'test_runtime': 20.9172, 'test_samples_per_second': 23.904, 'test_steps_per_second': 1.53}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     1.0000    0.0714    0.1333        14\n",
      "                   Account clarification     0.3750    0.2308    0.2857        13\n",
      "                          Bank statement     0.2429    0.8947    0.3820        19\n",
      "                            Calculations     0.0000    0.0000    0.0000        15\n",
      "                               Call-back     0.0000    0.0000    0.0000        13\n",
      "                            Cancellation     0.9286    0.7647    0.8387        17\n",
      "                    Change customer data     0.6296    0.9444    0.7556        18\n",
      "          Change in payment transactions     0.4444    0.8421    0.5818        19\n",
      "                       Claims management     0.0000    0.0000    0.0000        12\n",
      "                    Compensation release     0.6154    0.5000    0.5517        16\n",
      "                               Complaint     0.2308    0.2143    0.2222        14\n",
      "                     Contract adjustment     0.0000    0.0000    0.0000        15\n",
      "                      Contract rewriting     0.0000    0.0000    0.0000        17\n",
      "                             Credit item     0.0000    0.0000    0.0000        14\n",
      "           Damage report/repair approval     0.0000    0.0000    0.0000        16\n",
      "                            Data extract     0.8750    0.7368    0.8000        19\n",
      "                           Devinculation     0.0000    0.0000    0.0000        10\n",
      "                     Duplicate/KSV block     0.0000    0.0000    0.0000        14\n",
      "             FB Lustum/ beneficial owner     0.1311    0.9412    0.2302        17\n",
      "                Financing balance credit     0.0000    0.0000    0.0000        14\n",
      "Financing balance residual value leasing     1.0000    0.0714    0.1333        14\n",
      "                      Follow-up Contract     0.0000    0.0000    0.0000        15\n",
      "                   Fuel card/maintenance     0.0000    0.0000    0.0000        15\n",
      "            General contract information     0.0000    0.0000    0.0000         7\n",
      "                  General enquiry/errors     0.0000    0.0000    0.0000        11\n",
      "                                HB & HBO     0.0000    0.0000    0.0000        13\n",
      "                        Insurance change     0.0000    0.0000    0.0000        15\n",
      "                         Insurance offer     0.4750    1.0000    0.6441        19\n",
      "                       Liability Insurer     0.0000    0.0000    0.0000        13\n",
      "                Policy/endorsement/cover     0.6207    0.9474    0.7500        19\n",
      "                              Sanierung      0.0000    0.0000    0.0000        11\n",
      "                    Third party purchase     0.3542    1.0000    0.5231        17\n",
      "                             Unsubscribe     0.2742    1.0000    0.4304        17\n",
      "                             Vinculation     0.0000    0.0000    0.0000         8\n",
      "\n",
      "                                accuracy                         0.3600       500\n",
      "                               macro avg     0.2411    0.2988    0.2136       500\n",
      "                            weighted avg     0.2730    0.3600    0.2547       500\n",
      "\n",
      "Model, tokenizer and label_encoder saved to gelectra_email_classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "PREPARED_DIR = \"prepared_data\" \n",
    "MODEL_NAME = \"deepset/gelectra-base\"\n",
    "OUTPUT_DIR = \"gelectra_email_classifier\"\n",
    "MAX_LENGTH = 256\n",
    "RANDOM_SEED = 42\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "print(\"Loaded train/test shapes:\", train_df.shape, test_df.shape)\n",
    "print(\"Classes in encoder:\", len(label_encoder.classes_))\n",
    "\n",
    "if train_df['label'].dtype != int and not np.issubdtype(train_df['label'].dtype, np.integer):\n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "if test_df['label'].dtype != int and not np.issubdtype(test_df['label'].dtype, np.integer):\n",
    "    test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(\"num_labels =\", num_labels)\n",
    "print(\"Train label distribution:\\n\", train_df['label'].value_counts().sort_index().head(30))\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "if \"label\" in hf_train.column_names:\n",
    "    hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "if \"label\" in hf_test.column_names:\n",
    "    hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "cols_to_remove = [c for c in hf_train.column_names if c not in keep_cols]\n",
    "if cols_to_remove:\n",
    "    hf_train = hf_train.remove_columns(cols_to_remove)\n",
    "    hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Train columns after preprocess:\", hf_train.column_names)\n",
    "print(\"Test columns after preprocess:\", hf_test.column_names)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.eval()\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hf_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "loader = DataLoader(hf_train, batch_size=8)\n",
    "batch = next(iter(loader))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "print(out.logits.shape)\n",
    "if out.logits.shape[1] != num_labels:\n",
    "    raise RuntimeError(\"logits second dim != num_labels; check num_labels and model init\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    warmup_ratio=0.06,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "print(\"\\nClassification report on TEST:\")\n",
    "print(classification_report(hf_test[\"labels\"], preds, target_names=label_encoder.classes_, digits=4))\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "print(\"Model, tokenizer and label_encoder saved to\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
