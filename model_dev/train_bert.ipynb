{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64f6650d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 8446.68 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 10952.90 examples/s]\n",
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1689\n",
      "  Number of trainable parameters = 109953826\n",
      "  0%|          | 0/1689 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 30%|██▉       | 500/1689 [32:35<1:10:02,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5391, 'learning_rate': 1.4079336885731204e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 563/1689 [36:14<55:22,  2.95s/it]  The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "                                                  \n",
      " 33%|███▎      | 563/1689 [37:15<55:22,  2.95s/it]Saving model checkpoint to ./results/checkpoint-563\n",
      "Configuration saved in ./results/checkpoint-563/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5100831985473633, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 61.0321, 'eval_samples_per_second': 8.192, 'eval_steps_per_second': 1.032, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-563/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-563/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-563/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 59%|█████▉    | 1000/1689 [59:46<28:44,  2.50s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5218, 'learning_rate': 8.158673771462404e-06, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 1126/1689 [1:05:00<20:08,  2.15s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "                                                     \n",
      " 67%|██████▋   | 1126/1689 [1:05:44<20:08,  2.15s/it]Saving model checkpoint to ./results/checkpoint-1126\n",
      "Configuration saved in ./results/checkpoint-1126/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5064492225646973, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 44.7585, 'eval_samples_per_second': 11.171, 'eval_steps_per_second': 1.408, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1126/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1126/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1126/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 89%|████████▉ | 1500/1689 [1:21:20<07:49,  2.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5147, 'learning_rate': 2.238010657193606e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1689/1689 [1:29:10<00:00,  2.15s/it]The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "                                                     \n",
      "100%|██████████| 1689/1689 [1:29:53<00:00,  2.15s/it]Saving model checkpoint to ./results/checkpoint-1689\n",
      "Configuration saved in ./results/checkpoint-1689/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.50480318069458, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 43.6909, 'eval_samples_per_second': 11.444, 'eval_steps_per_second': 1.442, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./results/checkpoint-1689/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1689/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1689/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-563 (score: 0.0021534625410857986).\n",
      "100%|██████████| 1689/1689 [1:29:56<00:00,  3.20s/it]\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 8\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5396.4909, 'train_samples_per_second': 2.502, 'train_steps_per_second': 0.313, 'train_loss': 3.524956874271481, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [00:42<00:00,  1.47it/s]\n",
      "Configuration saved in gbert_email_classifier/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.5100831985473633, 'eval_accuracy': 0.038, 'eval_f1_macro': 0.0021534625410857986, 'eval_runtime': 43.4857, 'eval_samples_per_second': 11.498, 'eval_steps_per_second': 1.449, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model saved to: gbert_email_classifier\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "\n",
    "model_name = \"deepset/gbert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(label_encoder.classes_)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "test_ds = Dataset.from_pandas(test_df)\n",
    "train_ds = train_ds.map(preprocess_function, batched=True)\n",
    "test_ds = test_ds.map(preprocess_function, batched=True)\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "save_dir = \"gbert_email_classifier\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "joblib.dump(label_encoder, os.path.join(save_dir, \"label_encoder.pkl\"))\n",
    "\n",
    "print(f\"Trained model saved to: {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9661156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.71      0.65        14\n",
      "           1       0.62      0.62      0.62        13\n",
      "           2       0.71      0.53      0.61        19\n",
      "           3       0.79      0.73      0.76        15\n",
      "           4       0.77      0.77      0.77        13\n",
      "           5       0.88      0.88      0.88        17\n",
      "           6       0.60      0.83      0.70        18\n",
      "           7       0.76      0.84      0.80        19\n",
      "           8       0.71      0.83      0.77        12\n",
      "           9       0.75      0.75      0.75        16\n",
      "          10       0.43      0.43      0.43        14\n",
      "          11       0.92      0.73      0.81        15\n",
      "          12       0.92      0.71      0.80        17\n",
      "          13       0.56      0.71      0.62        14\n",
      "          14       0.85      0.69      0.76        16\n",
      "          15       0.81      0.68      0.74        19\n",
      "          16       0.69      0.90      0.78        10\n",
      "          17       0.71      0.71      0.71        14\n",
      "          18       0.78      0.82      0.80        17\n",
      "          19       0.57      0.57      0.57        14\n",
      "          20       0.54      0.50      0.52        14\n",
      "          21       0.76      0.87      0.81        15\n",
      "          22       0.79      0.73      0.76        15\n",
      "          23       1.00      0.43      0.60         7\n",
      "          24       0.83      0.91      0.87        11\n",
      "          25       0.67      0.62      0.64        13\n",
      "          26       1.00      0.67      0.80        15\n",
      "          27       0.62      0.84      0.71        19\n",
      "          28       0.90      0.69      0.78        13\n",
      "          29       0.82      0.74      0.78        19\n",
      "          30       0.69      0.82      0.75        11\n",
      "          31       0.60      0.88      0.71        17\n",
      "          32       0.92      0.65      0.76        17\n",
      "          33       0.71      0.62      0.67         8\n",
      "\n",
      "    accuracy                           0.72       500\n",
      "   macro avg       0.74      0.72      0.72       500\n",
      "weighted avg       0.74      0.72      0.72       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + LogisticRegression baseline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "vec = TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n",
    "X_train = vec.fit_transform(train_df['text'])\n",
    "X_test  = vec.transform(test_df['text'])\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "clf.fit(X_train, train_df['label'])\n",
    "pred = clf.predict(X_test)\n",
    "print(classification_report(test_df['label'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f34cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test shapes: (4500, 2) (500, 2)\n",
      "Classes in encoder: 34\n",
      "num_labels = 34\n",
      "Train label distribution:\n",
      " label\n",
      "0     130\n",
      "1     121\n",
      "2     171\n",
      "3     136\n",
      "4     115\n",
      "5     154\n",
      "6     159\n",
      "7     171\n",
      "8     111\n",
      "9     143\n",
      "10    127\n",
      "11    138\n",
      "12    150\n",
      "13    127\n",
      "14    140\n",
      "15    169\n",
      "16     86\n",
      "17    124\n",
      "18    159\n",
      "19    130\n",
      "20    130\n",
      "21    133\n",
      "22    137\n",
      "23     64\n",
      "24     96\n",
      "25    115\n",
      "26    133\n",
      "27    170\n",
      "28    114\n",
      "29    172\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading file vocab.txt from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/vocab.txt\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 7252.18 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 7513.50 examples/s]\n",
      "loading configuration file config.json from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"deepset/gbert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 31102\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/princess/.cache/huggingface/hub/models--deepset--gbert-base/snapshots/d50cb1df9660ff2de1af8a5362d322b3d5a1a28a/model.safetensors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n",
      "Test columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at deepset/gbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 4500\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2252\n",
      "  Number of trainable parameters = 109953826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity logits shape: torch.Size([8, 34])  expected second dim: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2252 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 22%|██▏       | 500/2252 [24:04<1:14:09,  2.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1415, 'learning_rate': 1.6559546313799624e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 563/2252 [26:43<1:00:29,  2.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      " 25%|██▌       | 563/2252 [27:23<1:00:29,  2.15s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-563\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-563/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6522839069366455, 'eval_accuracy': 0.69, 'eval_f1_macro': 0.6385645170624866, 'eval_runtime': 40.9014, 'eval_samples_per_second': 12.225, 'eval_steps_per_second': 0.782, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-563/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-563/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-563/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 44%|████▍     | 1000/2252 [45:38<51:37,  2.47s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2636, 'learning_rate': 1.1833648393194708e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1126/2252 [50:49<40:16,  2.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      " 50%|█████     | 1126/2252 [51:30<40:16,  2.15s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-1126\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-1126/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6033123731613159, 'eval_accuracy': 0.83, 'eval_f1_macro': 0.8158698550175251, 'eval_runtime': 40.8125, 'eval_samples_per_second': 12.251, 'eval_steps_per_second': 0.784, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-1126/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-1126/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-1126/special_tokens_map.json\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 67%|██████▋   | 1500/2252 [1:07:06<31:08,  2.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5546, 'learning_rate': 7.107750472589793e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 1689/2252 [1:14:58<20:25,  2.18s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      " 75%|███████▌  | 1689/2252 [1:15:39<20:25,  2.18s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-1689\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-1689/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.37991684675216675, 'eval_accuracy': 0.878, 'eval_f1_macro': 0.8729938101437713, 'eval_runtime': 40.9525, 'eval_samples_per_second': 12.209, 'eval_steps_per_second': 0.781, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-1689/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-1689/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-1689/special_tokens_map.json\n",
      "Deleting older checkpoint [gbert_email_classifier_fixed/checkpoint-563] due to args.save_total_limit\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 89%|████████▉ | 2000/2252 [1:28:39<10:24,  2.48s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3838, 'learning_rate': 2.3818525519848773e-06, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [1:39:06<00:00,  2.15s/it]***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "100%|██████████| 2252/2252 [1:39:47<00:00,  2.15s/it]Saving model checkpoint to gbert_email_classifier_fixed/checkpoint-2252\n",
      "Configuration saved in gbert_email_classifier_fixed/checkpoint-2252/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3391866385936737, 'eval_accuracy': 0.882, 'eval_f1_macro': 0.8818058479706438, 'eval_runtime': 41.194, 'eval_samples_per_second': 12.138, 'eval_steps_per_second': 0.777, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/checkpoint-2252/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/checkpoint-2252/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/checkpoint-2252/special_tokens_map.json\n",
      "Deleting older checkpoint [gbert_email_classifier_fixed/checkpoint-1126] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from gbert_email_classifier_fixed/checkpoint-2252 (score: 0.8818058479706438).\n",
      "100%|██████████| 2252/2252 [1:39:50<00:00,  2.66s/it]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5990.2027, 'train_samples_per_second': 3.005, 'train_steps_per_second': 0.376, 'train_loss': 1.222945096437715, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:40<00:00,  1.26s/it]\n",
      "Saving model checkpoint to gbert_email_classifier_fixed\n",
      "Configuration saved in gbert_email_classifier_fixed/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 0.3391866385936737, 'test_accuracy': 0.882, 'test_f1_macro': 0.8818058479706438, 'test_runtime': 41.7755, 'test_samples_per_second': 11.969, 'test_steps_per_second': 0.766}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     0.9333    1.0000    0.9655        14\n",
      "                   Account clarification     1.0000    0.7692    0.8696        13\n",
      "                          Bank statement     0.7143    0.7895    0.7500        19\n",
      "                            Calculations     0.9286    0.8667    0.8966        15\n",
      "                               Call-back     0.9286    1.0000    0.9630        13\n",
      "                            Cancellation     0.9412    0.9412    0.9412        17\n",
      "                    Change customer data     0.9474    1.0000    0.9730        18\n",
      "          Change in payment transactions     0.9048    1.0000    0.9500        19\n",
      "                       Claims management     0.9167    0.9167    0.9167        12\n",
      "                    Compensation release     0.8333    0.9375    0.8824        16\n",
      "                               Complaint     0.9286    0.9286    0.9286        14\n",
      "                     Contract adjustment     0.8235    0.9333    0.8750        15\n",
      "                      Contract rewriting     0.8889    0.9412    0.9143        17\n",
      "                             Credit item     0.8462    0.7857    0.8148        14\n",
      "           Damage report/repair approval     0.9231    0.7500    0.8276        16\n",
      "                            Data extract     0.9412    0.8421    0.8889        19\n",
      "                           Devinculation     1.0000    1.0000    1.0000        10\n",
      "                     Duplicate/KSV block     1.0000    0.9286    0.9630        14\n",
      "             FB Lustum/ beneficial owner     1.0000    0.9412    0.9697        17\n",
      "                Financing balance credit     0.5625    0.6429    0.6000        14\n",
      "Financing balance residual value leasing     0.8000    0.5714    0.6667        14\n",
      "                      Follow-up Contract     0.8667    0.8667    0.8667        15\n",
      "                   Fuel card/maintenance     0.9333    0.9333    0.9333        15\n",
      "            General contract information     1.0000    0.7143    0.8333         7\n",
      "                  General enquiry/errors     0.9000    0.8182    0.8571        11\n",
      "                                HB & HBO     0.7500    0.6923    0.7200        13\n",
      "                        Insurance change     1.0000    0.7333    0.8462        15\n",
      "                         Insurance offer     0.7391    0.8947    0.8095        19\n",
      "                       Liability Insurer     1.0000    0.9231    0.9600        13\n",
      "                Policy/endorsement/cover     0.7727    0.8947    0.8293        19\n",
      "                              Sanierung      1.0000    1.0000    1.0000        11\n",
      "                    Third party purchase     0.8095    1.0000    0.8947        17\n",
      "                             Unsubscribe     1.0000    1.0000    1.0000        17\n",
      "                             Vinculation     0.8750    0.8750    0.8750         8\n",
      "\n",
      "                                accuracy                         0.8820       500\n",
      "                               macro avg     0.8944    0.8774    0.8818       500\n",
      "                            weighted avg     0.8888    0.8820    0.8815       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in gbert_email_classifier_fixed/pytorch_model.bin\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/special_tokens_map.json\n",
      "tokenizer config file saved in gbert_email_classifier_fixed/tokenizer_config.json\n",
      "Special tokens file saved in gbert_email_classifier_fixed/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer and label_encoder saved to gbert_email_classifier_fixed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "PREPARED_DIR = \"prepared_data\" \n",
    "MODEL_NAME = \"deepset/gbert-base\"\n",
    "OUTPUT_DIR = \"gbert_email_classifier_fixed\"\n",
    "MAX_LENGTH = 256\n",
    "RANDOM_SEED = 42\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "print(\"Loaded train/test shapes:\", train_df.shape, test_df.shape)\n",
    "print(\"Classes in encoder:\", len(label_encoder.classes_))\n",
    "\n",
    "if train_df['label'].dtype != int and not np.issubdtype(train_df['label'].dtype, np.integer):\n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "if test_df['label'].dtype != int and not np.issubdtype(test_df['label'].dtype, np.integer):\n",
    "    test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(\"num_labels =\", num_labels)\n",
    "print(\"Train label distribution:\\n\", train_df['label'].value_counts().sort_index().head(30))\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "if \"label\" in hf_train.column_names:\n",
    "    hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "if \"label\" in hf_test.column_names:\n",
    "    hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "cols_to_remove = [c for c in hf_train.column_names if c not in keep_cols]\n",
    "if cols_to_remove:\n",
    "    hf_train = hf_train.remove_columns(cols_to_remove)\n",
    "    hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Train columns after preprocess:\", hf_train.column_names)\n",
    "print(\"Test columns after preprocess:\", hf_test.column_names)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.eval()\n",
    "sample = hf_train.select(range(min(8, len(hf_train)))).with_format(\"pt\")\n",
    "batch = {k: sample[k] for k in (\"input_ids\", \"attention_mask\")}\n",
    "with torch.no_grad():\n",
    "    out = model(**batch)\n",
    "print(\"Sanity logits shape:\", out.logits.shape, \" expected second dim:\", num_labels)\n",
    "if out.logits.shape[1] != num_labels:\n",
    "    raise RuntimeError(\"logits second dim != num_labels; check num_labels and model init\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    warmup_ratio=0.06,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "print(\"\\nClassification report on TEST:\")\n",
    "print(classification_report(hf_test[\"labels\"], preds, target_names=label_encoder.classes_, digits=4))\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "print(\"Model, tokenizer and label_encoder saved to\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train/test shapes: (4500, 2) (500, 2)\n",
      "Classes in encoder: 34\n",
      "num_labels = 34\n",
      "Train label distribution:\n",
      " label\n",
      "0     130\n",
      "1     121\n",
      "2     171\n",
      "3     136\n",
      "4     115\n",
      "5     154\n",
      "6     159\n",
      "7     171\n",
      "8     111\n",
      "9     143\n",
      "10    127\n",
      "11    138\n",
      "12    150\n",
      "13    127\n",
      "14    140\n",
      "15    169\n",
      "16     86\n",
      "17    124\n",
      "18    159\n",
      "19    130\n",
      "20    130\n",
      "21    133\n",
      "22    137\n",
      "23     64\n",
      "24     96\n",
      "25    115\n",
      "26    133\n",
      "27    170\n",
      "28    114\n",
      "29    172\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4500/4500 [00:00<00:00, 11997.54 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 12689.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n",
      "Test columns after preprocess: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at deepset/gelectra-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 34])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/7m/yq3bdqw12mj3g98dqwm4yk180000gn/T/ipykernel_22527/893037519.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "  0%|          | 0/2252 [00:00<?, ?it/s]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 22%|██▏       | 500/2252 [17:42<1:08:29,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5046, 'grad_norm': 1.7636359930038452, 'learning_rate': 1.6559546313799624e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▌       | 563/2252 [20:52<57:15,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3837265968322754, 'eval_accuracy': 0.11, 'eval_f1_macro': 0.030499007490506935, 'eval_runtime': 44.9092, 'eval_samples_per_second': 11.134, 'eval_steps_per_second': 0.713, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 44%|████▍     | 1000/2252 [33:21<25:19,  1.21s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2722, 'grad_norm': 3.4851925373077393, 'learning_rate': 1.1833648393194708e-05, 'epoch': 1.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 50%|█████     | 1126/2252 [36:16<20:08,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.962937116622925, 'eval_accuracy': 0.256, 'eval_f1_macro': 0.14457666516490048, 'eval_runtime': 20.9314, 'eval_samples_per_second': 23.888, 'eval_steps_per_second': 1.529, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 67%|██████▋   | 1500/2252 [43:55<15:12,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9613, 'grad_norm': 6.684183120727539, 'learning_rate': 7.107750472589793e-06, 'epoch': 2.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      " 75%|███████▌  | 1689/2252 [48:05<10:03,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6687939167022705, 'eval_accuracy': 0.328, 'eval_f1_macro': 0.1853267007671209, 'eval_runtime': 20.8458, 'eval_samples_per_second': 23.986, 'eval_steps_per_second': 1.535, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      " 89%|████████▉ | 2000/2252 [54:26<05:05,  1.21s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7207, 'grad_norm': 4.933954238891602, 'learning_rate': 2.3818525519848773e-06, 'epoch': 3.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [59:32<00:00,  1.07s/it]/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "                                                   \n",
      "100%|██████████| 2252/2252 [59:57<00:00,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.5524351596832275, 'eval_accuracy': 0.36, 'eval_f1_macro': 0.21359333803229644, 'eval_runtime': 21.1401, 'eval_samples_per_second': 23.652, 'eval_steps_per_second': 1.514, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2252/2252 [59:59<00:00,  1.60s/it]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 3599.4051, 'train_samples_per_second': 5.001, 'train_steps_per_second': 0.626, 'train_loss': 3.0628380512894786, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:20<00:00,  1.59it/s]\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval metrics (HF): {'test_loss': 2.5524351596832275, 'test_accuracy': 0.36, 'test_f1_macro': 0.21359333803229644, 'test_runtime': 20.9172, 'test_samples_per_second': 23.904, 'test_steps_per_second': 1.53}\n",
      "\n",
      "Classification report on TEST:\n",
      "                                          precision    recall  f1-score   support\n",
      "\n",
      "                   1st Level Onlinetools     1.0000    0.0714    0.1333        14\n",
      "                   Account clarification     0.3750    0.2308    0.2857        13\n",
      "                          Bank statement     0.2429    0.8947    0.3820        19\n",
      "                            Calculations     0.0000    0.0000    0.0000        15\n",
      "                               Call-back     0.0000    0.0000    0.0000        13\n",
      "                            Cancellation     0.9286    0.7647    0.8387        17\n",
      "                    Change customer data     0.6296    0.9444    0.7556        18\n",
      "          Change in payment transactions     0.4444    0.8421    0.5818        19\n",
      "                       Claims management     0.0000    0.0000    0.0000        12\n",
      "                    Compensation release     0.6154    0.5000    0.5517        16\n",
      "                               Complaint     0.2308    0.2143    0.2222        14\n",
      "                     Contract adjustment     0.0000    0.0000    0.0000        15\n",
      "                      Contract rewriting     0.0000    0.0000    0.0000        17\n",
      "                             Credit item     0.0000    0.0000    0.0000        14\n",
      "           Damage report/repair approval     0.0000    0.0000    0.0000        16\n",
      "                            Data extract     0.8750    0.7368    0.8000        19\n",
      "                           Devinculation     0.0000    0.0000    0.0000        10\n",
      "                     Duplicate/KSV block     0.0000    0.0000    0.0000        14\n",
      "             FB Lustum/ beneficial owner     0.1311    0.9412    0.2302        17\n",
      "                Financing balance credit     0.0000    0.0000    0.0000        14\n",
      "Financing balance residual value leasing     1.0000    0.0714    0.1333        14\n",
      "                      Follow-up Contract     0.0000    0.0000    0.0000        15\n",
      "                   Fuel card/maintenance     0.0000    0.0000    0.0000        15\n",
      "            General contract information     0.0000    0.0000    0.0000         7\n",
      "                  General enquiry/errors     0.0000    0.0000    0.0000        11\n",
      "                                HB & HBO     0.0000    0.0000    0.0000        13\n",
      "                        Insurance change     0.0000    0.0000    0.0000        15\n",
      "                         Insurance offer     0.4750    1.0000    0.6441        19\n",
      "                       Liability Insurer     0.0000    0.0000    0.0000        13\n",
      "                Policy/endorsement/cover     0.6207    0.9474    0.7500        19\n",
      "                              Sanierung      0.0000    0.0000    0.0000        11\n",
      "                    Third party purchase     0.3542    1.0000    0.5231        17\n",
      "                             Unsubscribe     0.2742    1.0000    0.4304        17\n",
      "                             Vinculation     0.0000    0.0000    0.0000         8\n",
      "\n",
      "                                accuracy                         0.3600       500\n",
      "                               macro avg     0.2411    0.2988    0.2136       500\n",
      "                            weighted avg     0.2730    0.3600    0.2547       500\n",
      "\n",
      "Model, tokenizer and label_encoder saved to gelectra_email_classifier\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "PREPARED_DIR = \"prepared_data\" \n",
    "MODEL_NAME = \"deepset/gelectra-base\"\n",
    "OUTPUT_DIR = \"gelectra_email_classifier\"\n",
    "MAX_LENGTH = 256\n",
    "RANDOM_SEED = 42\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "set_seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(PREPARED_DIR, \"train.csv\"))\n",
    "test_df  = pd.read_csv(os.path.join(PREPARED_DIR, \"test.csv\"))\n",
    "label_encoder = joblib.load(os.path.join(PREPARED_DIR, \"label_encoder.pkl\"))\n",
    "\n",
    "print(\"Loaded train/test shapes:\", train_df.shape, test_df.shape)\n",
    "print(\"Classes in encoder:\", len(label_encoder.classes_))\n",
    "\n",
    "if train_df['label'].dtype != int and not np.issubdtype(train_df['label'].dtype, np.integer):\n",
    "    train_df['label'] = train_df['label'].astype(int)\n",
    "if test_df['label'].dtype != int and not np.issubdtype(test_df['label'].dtype, np.integer):\n",
    "    test_df['label'] = test_df['label'].astype(int)\n",
    "\n",
    "num_labels = len(label_encoder.classes_)\n",
    "print(\"num_labels =\", num_labels)\n",
    "print(\"Train label distribution:\\n\", train_df['label'].value_counts().sort_index().head(30))\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "hf_test  = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "hf_train = hf_train.map(preprocess_function, batched=True)\n",
    "hf_test  = hf_test.map(preprocess_function, batched=True)\n",
    "\n",
    "if \"label\" in hf_train.column_names:\n",
    "    hf_train = hf_train.rename_column(\"label\", \"labels\")\n",
    "if \"label\" in hf_test.column_names:\n",
    "    hf_test = hf_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "keep_cols = {\"input_ids\", \"attention_mask\", \"labels\"}\n",
    "cols_to_remove = [c for c in hf_train.column_names if c not in keep_cols]\n",
    "if cols_to_remove:\n",
    "    hf_train = hf_train.remove_columns(cols_to_remove)\n",
    "    hf_test  = hf_test.remove_columns([c for c in hf_test.column_names if c not in keep_cols])\n",
    "\n",
    "print(\"Train columns after preprocess:\", hf_train.column_names)\n",
    "print(\"Test columns after preprocess:\", hf_test.column_names)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "model.eval()\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "hf_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "loader = DataLoader(hf_train, batch_size=8)\n",
    "batch = next(iter(loader))\n",
    "with torch.no_grad():\n",
    "    out = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "print(out.logits.shape)\n",
    "if out.logits.shape[1] != num_labels:\n",
    "    raise RuntimeError(\"logits second dim != num_labels; check num_labels and model init\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE * 2,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1_macro\",\n",
    "    greater_is_better=True,\n",
    "    seed=RANDOM_SEED,\n",
    "    warmup_ratio=0.06,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"macro\")\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_test,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "pred_out = trainer.predict(hf_test)\n",
    "preds = np.argmax(pred_out.predictions, axis=-1)\n",
    "print(\"Final eval metrics (HF):\", pred_out.metrics)\n",
    "print(\"\\nClassification report on TEST:\")\n",
    "print(classification_report(hf_test[\"labels\"], preds, target_names=label_encoder.classes_, digits=4))\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "joblib.dump(label_encoder, os.path.join(OUTPUT_DIR, \"label_encoder.pkl\"))\n",
    "print(\"Model, tokenizer and label_encoder saved to\", OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
